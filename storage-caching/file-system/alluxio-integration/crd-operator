#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Futurewei Technologies.
# Copyright: All rights reserved (2022+)
# Author: Nikunj J Parekh
# Email: nparekh@futurewei.com

import json
import logging
import os
import requests
import subprocess as sp
import sys
import time
import threading

from cache_manager import CacheManagerABC
from cache_manager import ExecutorServiceABC
from multiprocessing import active_children

# Declare const strings
NAMESPACE = os.getenv("res_namespace", "default")
ALLUXIO_BASE_PATH = "/futurewei-data/datasets"
CR_EXTENSION = "autogenerated-cr"
K8S_API_PORT = 8002
K8S_API_BASE_URL = f"http://127.0.0.1:{K8S_API_PORT}"
PODS_URI = f"api/v1/namespaces/{NAMESPACE}"
DEPLOYMENTS_URI = f"apis/apps/v1/namespaces/{NAMESPACE}"
JOBS_URI = f"apis/batch/v1/namespaces/{NAMESPACE}"
RESOURCE_URI_MAP = dict(deployments=DEPLOYMENTS_URI, pods=PODS_URI, jobs=JOBS_URI)
ALLUXIO_ROOT_FUSE_MOUNT="/journal"

# Setup logging
log = logging.getLogger(__name__)
logging.basicConfig(
    format=" [%(asctime)s] | %(levelname)s\t|  %(message)s", level=logging.INFO
)


def warn(*foo):
    log.warning(*foo)


# Class definitions
class DatasetCacheManagerBase(CacheManagerABC):
    def __init__(self):
        pass

    def _print_banner(self):
        log.info("-" * 50)
        log.info("Futurewei Technologies, Inc.")
        log.info("Copyright: All rights reserved (2022+)")
        log.info("Author: Nikunj J Parekh")
        log.info("Email: nparekh@futurewei.com")
        log.info("Dataset Caching Operator started")
        log.info("-" * 50 + "\n")

    def copy_data_to_datasetcache(self, *vargs):
        pass

    def check_caching_required(self, spec):

        need_caching = False

        volumes = spec.get("volumes", [])

        for v in volumes:
            need_caching = (
                True
                if "hostPath" in v
                else log.debug(
                    f"No hostPath for volume spec {v} in {self._deploy_name}"
                )
            )
            if need_caching:
                return True

        if "alnair-datacache-operator" not in self._deploy_name:
            log.debug(
                f"Dataset hydration is not required for resource {self._deploy_name}"
            )

        return False

    def perform_data_caching(self, *vargs):
        pass


# Exception handlers
class DatasetCacheError(Exception):
    pass


# Manager class to handle copying and fairly distributing the data across all workers roughly equally

class MasterNodeTaskExecutor(ExecutorServiceABC):

    """
    Manager Class to handle copying and fairly distributing the data across all workers roughly equally
    """

    @classmethod
    def get_space_needed_for_dataset_in_bytes(cls, dataset_path):
        """This method measures the returns the total space required to store the given dataset."""

        # Get the first worker node, always

        host_list_cmd = "kubectl get nodes -o jsonpath='{$.items[*].status.addresses[?(@.type==\"InternalIP\")].address}'"

        host_names = sp.run(
            host_list_cmd.split(), stdout=sp.PIPE, check=True
        ).stdout.decode("utf-8")

        # Arbitrarily picking first worker node, and assuming its the second row in response to kubectl get nodes command
        host_name = host_names.split()[1]

        if not host_name:
            raise DatasetCacheError(
                "Dataset Cache operator failed to get the IP address of the first worker to access dataset"
            )

        try:
            val = (
                sp.run(
                    ("ssh nikunj@" + host_name + " du -s " + dataset_path).split(),
                    stdout=sp.PIPE,
                    check=True,
                )
                .stdout.decode("utf-8")
                .split()[0]
            )

        except sp.CalledProcessError as e:
            log.error(
                f"Unable to connect to Data Cache Master to estimate the size of dataset at {dataset_path}:"
            )
            log.error(str(e))
            raise

        return int(val) * 1000 if val.isdigit() else sys.maxsize


    # Tha main method that copies the data into the Alluxio in-memory distributed cache cluster
    # This method assumes that all checks and validations are already done.
    @classmethod
    def get_capacity_bytes(cls):
        """This method returns the total capacity of in-memory Alluxio cache cluster"""

        try:
            val = (
                sp.run(
                    (
                        "kubectl exec -it alluxio-master-0 -c alluxio-master -n "
                        + NAMESPACE
                        + " -- alluxio fs getCapacityBytes"
                    ).split(),
                    stdout=sp.PIPE,
                    check=True,
                )
                .stdout.decode("utf-8")
                .split()[-1]
            )

        except sp.CalledProcessError as e:
            log.error(
                f"Unable to connect to Data Cache Master to estimate the total cache capacity before copying dataset:"
            )
            log.error(str(e))
            raise

        return int(val) if val.isdigit() else -sys.maxsize

    # This method returns the total size of used bytes of in-memory Alluxio cache cluster

    @classmethod
    def get_used_bytes(cls):
        """This method returns the total size of used bytes of in-memory Alluxio cache cluster"""

        try:
            val = (
                sp.run(
                    (
                        "kubectl exec -it alluxio-master-0 -c alluxio-master -n "
                        + NAMESPACE
                        + " -- alluxio fs getUsedBytes"
                    ).split(),
                    stdout=sp.PIPE,
                    check=True,
                )
                .stdout.decode("utf-8")
                .split()[-1]
            )

        except sp.CalledProcessError as e:
            log.error(
                f"Unable to connect to Data Cache Master to estimate the size of cache available before copying dataset:"
            )
            log.error(str(e))
            raise

        return int(val) if val.isdigit() else sys.maxsize


# Manager class to handle copying the data on to a single provided worker

class WorkerNodeTaskExecutor(ExecutorServiceABC):

    """
    Manager class to handle copying the data onto the provided workers
    """

    def __init__(self, node_name):
        self._nodename = node_name

    # This method measures the returns the total space required to store the given dataset.

    def get_space_needed_for_dataset_in_bytes(self, dataset_path):
        """This method measures the returns the total space required to store the given dataset."""

        try:
            val = (
                sp.run(
                    ("ssh nikunj@" + self._nodename + " du -s " + dataset_path).split(),
                    stdout=sp.PIPE,
                    check=True,
                )
                .stdout.decode("utf-8")
                .split()[0]
            )

        except sp.CalledProcessError as e:
            log.error(
                f"Unable to connect to Data Cache worker {self._nodename} to estimate the size of cache available before copying dataset:"
            )
            log.error(str(e))
            raise

        # raise sp.CalledProcessError if data is not present or
        # there are other errors connectinng to the worker.

        return int(val) * 1000 if val.isdigit() else sys.maxsize

    # This method returns the total capacity and size of used bytes
    # of in-memory Alluxio cache cluster

    def get_capacity_and_used_bytes_onworker(self):
        """
        This method returns the total capacity and size of used bytes
        of in-memory Alluxio cache cluster
        """

        try:
            # Get number of Alluxio worker running on this node

            _TMP_NUM_WORKERS_ = 2 # TODO: future work

            capacity = MasterNodeTaskExecutor.get_capacity_bytes() / _TMP_NUM_WORKERS_
            used = MasterNodeTaskExecutor.get_used_bytes() / _TMP_NUM_WORKERS_

            log.info(
                f"Returning estimated total capacity = {capacity/1e6} MB and used = {used/1e6} MB for worker {self._nodename}"
            )
            return capacity, used

        except Exception as e:
            log.error("Unable to get total capacity and used capacity on worker node:")
            log.error(f"  {str(e)}")

        return -1, -1


# Alluxio Cache Manager
# The class responsible for copying data into Alluxio when a deployment/job/pod etc
# have the `cacheDataset` annotation set to "yes" with quotes.

class AlluxioCacheManager(DatasetCacheManagerBase):

    """
    Alluxio Cache Manager
    The class responsible for copying data into Alluxio when a deployment/job/pod etc
    have the `cacheDataset` annotation set to "yes" with quotes.
    """

    def __init__(self):

        self._print_banner()
        self._deploy_name = ""
        self._output_filename = ""
        self._daemons = [None] * 3

    def _print_banner(self):
        super()._print_banner()

    # This method returns the total capacity of in-memory Alluxio cache cluster
    def copy_data_to_datasetcache(self, datasetdir_basename, dataset_path, resource_rev):

        """Tha main method that copies the data into the Alluxio in-memory distributed cache cluster
        This method assumes that all checks and validations are already done."""

        cp_cmd = (
            "kubectl exec -it alluxio-master-0 -c alluxio-master -n "
            + NAMESPACE
            + " -- alluxio fs copyFromLocal --thread 64 "
            + ALLUXIO_ROOT_FUSE_MOUNT
            + datasetdir_basename  # /journal/datasets/<host_path without /mnt/fuse: sort of experiment path>
            + " "
            + dataset_path
        )

        try:
            out = sp.run(
                cp_cmd.split(), stdout=sp.PIPE, stderr=sp.PIPE, check=True, timeout=None
            )

            log.debug(f"\tCommand: {cp_cmd}")
            log.debug(f"Stdout of the hydration command: {out.stdout}")
            log.debug(f"Stderr of the hydration command: {out.stderr}")
            log.info(
                f"Data hydration completed for {self._deploy_name}, mount {datasetdir_basename}, path {dataset_path}, rev: {resource_rev})"
            )

        except Exception as e:  # Also captures CalledProcessError when actual cmd fails
            warn(
                f"Data hydration request may have been already satisfied. Check if data is already cached by browsing to the dashboard of Dataset Cache master."
            )

            return False

        return True

    def _delete_original_resource(self, resource):
        """This method deletes original deployment/job/pod once the custom resource is confirmed to have been created."""

        log.debug(
            f"Now deleting {resource} {self._deploy_name} and creating equivalent AlnairDatacache deployment"
        )

        del_cmd = (
            # f"kubectl delete {resource} {self._deploy_name} --wait=true --now=true --timeout=10s -n "
            f"kubectl delete {resource} {self._deploy_name} --wait=false --now=true -n "
            + NAMESPACE
        )

        log.debug(f"DEBUG: Delete cmd: {del_cmd}")

        try:
            sp.run(del_cmd.split(), check=True, timeout=60)

        except sp.CalledProcessError as e:
            log.error(f"Failed to delete original resource {self._deploy_name}:")
            log.error(str(e))

    def check_caching_required(self, spec):

        return super().check_caching_required(spec)


    def _create_alnair_datacache_custom_resource(self, resource):
        """This method creates the custome resource in lieu of the original deployment/job/pod"""

        # TODO: Use Api server and sidecar proxy - provided it works.. currently it doesn't return resource spec
            # K8S API DOES NOT WORK AS ADVERTISED in version 1.21 till 1.25.
            # api_inst.create_namespaced_custom_object(
            # group="centaurusinfra.com", version="v1",
            # namespace=NAMESPACE, plural="datacaches", body=data)

        create_cr_cmd = f"/create-alnair-datacache-resource {resource} {self._deploy_name} {CR_EXTENSION}"

        try:
            return (
                sp.run(create_cr_cmd.split(), stderr=None, timeout=60).returncode == 0
            )

        except Exception as e:
            log.error(
                f"Deployment creation program failed to create AlnairDatacache deployment:"
            )
            log.error(str(e))

        return False


    def perform_data_caching(
        self, spec, scheduled_nodeName, worker_executor, resource_rev
    ):

        volumes = spec.get("volumes", [])

        # Collect hostPath spec for all PVs (persistent volumes) this container uses
        for v in volumes:

            # Build from and to paths for copying data

            if "hostPath" not in v:
                log.debug(
                    f"No hostPath for volume spec {v}.. checking next volumeMount.."
                )
                continue

            vname, host_path = v["name"], v["hostPath"]["path"]

            # chomp trailing /
            if host_path[-1] == "/":
                host_path = host_path[:-1]

            # host_path is expected to be already in /mnt/fuse or /mnt/fuse3 location ie,
            # in the format /mnt/fuse3/alluxio-journal/<experiment_name>.
            # We need to extract the experiment name, we believe it'd be the leaf level dir name.
            # Also supports host_path with /nfs.

            # Now build the path inside Alluxio data cache
            experiment_leaf_dir = host_path.split("/")[-1]  # experiment name
            dataset_path_in_cache = os.path.join(ALLUXIO_BASE_PATH, experiment_leaf_dir)

            # Get unused size and capacity of Dataset Cache
            if scheduled_nodeName:
                log.info(
                    f"Node affinity is identified for in-progress deployment {self._deploy_name} with node {scheduled_nodeName}"
                )

                capacity, used = worker_executor.get_capacity_and_used_bytes_onworker()

                new_data_size = worker_executor.get_space_needed_for_dataset_in_bytes(
                    host_path
                    # Since affinity is specified, the to-be-cached data is available only on
                    # one of the alluxio worker pods, at this path after caching:
                    # /mnt/alluxio-fuse/futurewei-data/datasets/<experiment_name>
                    # but only after data copy happens using copyFromLocal
                    # hostPath: /mnt/fuse3/alluxio-journal/<experiment_name>
                )

            else:
                capacity      = MasterNodeTaskExecutor.get_capacity_bytes()
                used          = MasterNodeTaskExecutor.get_used_bytes()
                new_data_size = MasterNodeTaskExecutor.get_space_needed_for_dataset_in_bytes(
                    host_path
                )

            log.debug(f"Total size of new data: {int(new_data_size / 1e6)} MB")
            log.debug(f"Total cache available: {int((capacity - used)/ 1e6)} MB")
            log.debug(
                f"Total space required (10% margin): {int(1.1 * new_data_size / 1e6)} MB"
            )

            # Leave 10% margin of empty cache, for now

            if capacity - used >= 1.1 * new_data_size:

                # TODO: Find a way to "claim" or block before copying
                log.debug(
                    "Dataset Cache has sufficient capacity to copy dataset. Starting data hydration"
                )
                log.info(
                    f"Hydrating cache with data from {host_path} to {dataset_path_in_cache}"
                )

                # /mnt/fuse part gets removed, gets rest of path
                datasetdir_basename = "/".join(host_path.split("/")[4:])

                success = self.copy_data_to_datasetcache(
                    datasetdir_basename, dataset_path_in_cache, resource_rev
                )

                if success:
                    log.info(f"Total cache available: {int((capacity - used)/1e6)} MB")

            else:
                warn(
                    "Data hydration request is ignored due to lack of available cache capacity."
                )
                warn(f"  Cache capacity used:      {used}")
                warn(f"  Cache capacity available: {capacity-used}")
                warn(f"  Cache capacity required:  {1.1*new_data_size}")


    def _deploy_alnair_datacache_custom_resource(self, resource):

        log.info(
            f"Now deploying new custom resource {self._deploy_name}-{CR_EXTENSION}"
        )

        # Create / Replace the deployment
        # deploy_cmd = f"kubectl create -f {self._output_filename} --validate=false"
        deploy_cmd = (
            f"kubectl replace --force -f {self._output_filename} --validate=false"
        )

        try:
            sp.run(
                deploy_cmd.split(),
                check=True,
                stdout=sp.PIPE,
                stderr=sp.PIPE,
                timeout=60,
            )

            self._mark_done()

        except sp.CalledProcessError as e:
            warn("Deployment failed for AlnairDatacache resource:")
            warn(f"  Command: {deploy_cmd}")
            warn(
                "  Please check if the corresponding AlnairDatacache deployment is already running"
            )

    def _mark_done(self):
        try:
            mv_cmd = f"mv {self._output_filename} {self._output_filename}.done"
            log.debug(f"Moving the {self._output_filename} to .done..")
            sp.run(
                mv_cmd.split(),
                check=False,
                stdout=sp.DEVNULL,
                stderr=sp.DEVNULL,
                timeout=10,
            )
        except:
            pass


    # This method processes the fields of the newly deploy-ing resource object to check for
    # the "cacheDataset" attribute. If this attribute is found, the methods triggers
    # copying of the dataset available on hostPath location, into the Alluxio in-memory cache.

    def process_new_resource_object(self, resource, obj, resource_rev):
        """
        This method processes the fields of the newly deploy-ing resource object to check for
        the "cacheDataset" attribute. If this attribute is found, the methods triggers
        copying of the dataset available on hostPath location, into the Alluxio in-memory cache.
        a hostPath can be a local dir, or an NFS volume, or a S3 mount or any other mount point, and
        no change is required to this code.
        Necessary conditions for dataset caching to trigger are:
        1) annotation cacheDataset: "yes" (and not "no" etc), must be present in metadata spec.
        2) a volume.path and a volumeMount.mountPath must be declared with valid .path and .mountPath.

        Also, if a nodeName or a pod affinity/anti-affinity or a pod topology spec is present
        in the event, it'd be considered an affinity spec and all of the data will be copied over into
        that worker.
        """

        event_type = obj.get("type")

        if not event_type:
            return False, None

        if event_type not in ("MODIFIED", "ADDED"):  # "DELETED" ignored
            return False, None

        success = False

        metadata = obj.get("object").get("metadata")

        if not metadata or "deletionTimestamp" in metadata:
            return False, None

        try:

            self._deploy_name = metadata.get("name")

            self._output_filename = (
                f"datacache-custom-resource-{self._deploy_name}-{resource}.yml"
            )

            annotations = metadata.get("annotations")
            do_cache_dataset = (
                annotations.get("cacheDataset", "no").casefold() == "yes"
                if annotations
                else False
            )

            if resource == "deployments":
                spec = (
                    obj.get("object", {})
                    .get("spec", {})
                    .get("template", {})
                    .get("spec", {})
                )
            elif resource == "pods":
                spec = obj.get("object", {}).get("spec", {})
            elif resource == "jobs":
                spec = obj.get("object", {}).get("spec", {})

            if not spec or not self.check_caching_required(spec):
                return False

            scheduled_nodeName = None  # spec.get("nodeName")

            worker_executor = (
                WorkerNodeTaskExecutor(scheduled_nodeName)
                if scheduled_nodeName
                else None
            )

            # TODO: Future work - handle other ways to specify affinity, and anti-affinity

            if do_cache_dataset:

                # Since this new deployment has cacheDataset="yes", absorb / read and delete that deployment/pod.

                if self._create_alnair_datacache_custom_resource(resource):

                    log.info(
                        f"Dataset hydration requested by {resource}: {self._deploy_name} for event rev: {resource_rev}"
                    )
                    log.info(
                        f"Created new AlnairDatacache deployment for {resource} {self._deploy_name}"
                    )

                    self._delete_original_resource(resource)

                    self.perform_data_caching(
                        spec, scheduled_nodeName, worker_executor, resource_rev
                    )

                    self._deploy_alnair_datacache_custom_resource(resource)

                    log.info(
                        f"Completed custom resource creation for resource: {self._deploy_name}\n"
                    )
                    success = True

                else:
                    self._mark_done()

        except Exception as e:
            log.error(
                f"Data hydration is not possible for resource {self._deploy_name} due to errors:"
            )
            log.error(str(e))

        worker_executor = None

        return success


    def _stop_daemons(self):
        for d in self._daemons:
            d.join()
        self._daemons = []


    # This is the main function that watches the Kubernetes API for changes as follows:
    # This method implements the main event loop to frequently check the kubernetes cluster for
    # any new deployments. If a new deployment or pod or job is detected, the method triggers
    # the process to check for need of Dataset Caching and copies the dataset into the Alluxio cache
    # if doing so is possible. """

    def run(self, resource):
        # Do not use kubernetes library module, its still flaky. sp is ok

        """This method implements the main event loop to frequently check the kubernetes cluster for
        any new deployments. If a new deployment or pod or job is detected, the method triggers
        the process to check for need of Dataset Caching and copies the dataset into the Alluxio cache
        if doing so is possible."""

        # We have to do this because resourceVersion and resourceVersionMatch features in K8s do not work as advertised

        resource_vers = set()

        log.info(
            f"Checking for new resources and updates to {resource} that need Dataset Cache to be hydrated..."
        )

        url = f"{K8S_API_BASE_URL}/{RESOURCE_URI_MAP[resource]}/{resource}?watch=true"

        while True:

            # Request the API endpoint and keep conenction open

            try:
                log.debug(f"\turl: {url}")
                response = requests.get(url, stream=True)
                log.debug("\tresponse recd")

            except requests.exceptions.HTTPError as e:
                warn(
                    f"An unrecognized response while querying {url} is ignored: {str(e)}, {type(e)}"
                )

            try:
                if response.ok:
                    for line in response.iter_lines():
                        obj = json.loads(line)

                        new_version = (
                            obj.get("object", {})
                            .get("metadata", {})
                            .get("resourceVersion", None)
                        )

                        if new_version and new_version not in resource_vers:

                            log.debug(f"{resource_vers}")

                            success = self.process_new_resource_object(
                                resource, obj, new_version
                            )

                            if success:

                                if len(resource_vers) > 10:
                                    resource_vers.clear()

                                resource_vers.add(new_version)
                else:
                    log.debug("response != ok")

            except DatasetCacheError as e:
                warn(
                    f"Unrecognized deployment object '{response}' is ignored: {str(e)}, {type(e)}"
                )

            time.sleep(2)

    def stop(self):

        self._stop_daemons()

        # get all active child processes
        children = active_children()

        warn(f"Terminating any active Children: {len(children)}")
        for c in children:
            c.terminate()  # Kill gracefully, keep children

    def abort(self):

        self._stop_daemons()

        # get all active child processes
        children = active_children()

        warn(f"Abort any active Children: {len(children)}")
        for c in children:
            c.kill()  # Kill immediately, kill any children as well

    def start(self):

        log.debug("Inside top level start of CacheManager..")

        self._daemons = [
            threading.Thread(
                daemon=True,
                target=self.run,
                args=(resource,),
                name=f"{resource} processor",
            )
            for resource in RESOURCE_URI_MAP
        ]

        for d in self._daemons:
            d.start()

        # Main loop; does nothing
        while True:
            time.sleep(100)


def init_proxy_task():

    k8s_proxy_cmd = f"/deploy-proxy {K8S_API_PORT} {ALLUXIO_BASE_PATH}"

    try:
        sp.run(k8s_proxy_cmd.split(),
            stdout=sp.PIPE, stderr=sp.PIPE, check=True, timeout=None
        )

    except sp.CalledProcessError as e:
        log.exception("Proxy deployment failed")
        raise


try:
    # Instantiate the AlluxioCacheManager
    cache_manager = AlluxioCacheManager()

    # Run Kubernetes Proxy on port K8S_API_PORT
    proxy_daemon = threading.Thread(daemon=True, target=init_proxy_task, name="K8s_Proxy")
    proxy_daemon.start()

    time.sleep(3)  # Let the proxy server boot up
    warn("Node affinity is ignored for now")

    # Start the event loop
    cache_manager.start()

    log.info("Stopping Cache Manager...")
    cache_manager.stop()

    log.info("Stopping Kubernetes Proxy...")

    proxy_daemon.join()
    if proxy_daemon.isAlive():
        raise SystemExit()

except Exception as e:
    log.exception(f"{str(e)}")
    cache_manager.abort()
    raise SystemExit()
