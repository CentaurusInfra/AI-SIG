#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Futurewei Technologies.
# Copyright: All rights reserved (2022+)
# Author: Nikunj J Parekh
# Email: nparekh@futurewei.com

import json
import logging
import os

# import kubernetes.client as c
import requests
import subprocess as sp
import sys
import time
import threading

# import yaml

from cache_manager import CacheManagerABC
from cache_manager import ExecutorServiceABC
from multiprocessing import active_children

# Declare const strings
NAMESPACE = os.getenv("res_namespace", "default")
ALLUXIO_BASE_PATH = "/futurewei-data/datasets"
CR_EXTENSION = "autogenerated-cr"
K8S_API_PORT = 8002
K8S_API_BASE_URL = f"http://127.0.0.1:{K8S_API_PORT}"
PODS_URI = f"/api/v1/namespaces/{NAMESPACE}"
DEPLOYMENTS_URI = f"/apis/apps/v1/namespaces/{NAMESPACE}"
JOBS_URI = f"/apis/batch/v1/namespaces/{NAMESPACE}"
RESOURCE_URI_MAP = dict(deployments=DEPLOYMENTS_URI, pods=PODS_URI, jobs=JOBS_URI)


# Setup logging
log = logging.getLogger(__name__)
out_hdlr = logging.StreamHandler(sys.stdout)
out_hdlr.setFormatter(logging.Formatter("%(asctime)s %(message)s"))
out_hdlr.setLevel(logging.INFO)
log.addHandler(out_hdlr)
log.setLevel(logging.INFO)


class DatasetCacheManager(CacheManagerABC):
    pass


# Exception handlers
class DatasetCacheError(Exception):
    pass


# Manager class to handle copying the data on to a single provided worker


class MasterNodeTaskExecutor(ExecutorServiceABC):

    """
    Manager Class to handle copying and fairly distributing the data across all workers roughly equally
    """

    @classmethod
    def get_space_needed_for_dataset_in_bytes(cls, dataset_path):

        """This method measures the returns the total space required to store the given dataset."""

        # Get the first worker node, always

        # TODO: Use Api server and sidecar ..
        host_list_cmd = "kubectl get nodes -o jsonpath='{$.items[*].status.addresses[?(@.type==\"InternalIP\")].address}'"

        host_names = sp.run(
            host_list_cmd.split(),
            stdout=sp.PIPE,
        ).stdout.decode("utf-8")

        host_name = host_names.split()[1]

        if not host_name:
            raise DatasetCacheError(
                "Dataset Cache operator failed to get the IP address of the first worker to access dataset"
            )

        try:
            val = (
                sp.run(
                    ("ssh nikunj@" + host_name + " du -s " + dataset_path).split(),
                    stdout=sp.PIPE,
                )
                .stdout.decode("utf-8")
                .split()[0]
            )

        except sp.CalledProcessError as e:
            log.exception(
                f"ERROR: Unable to connect to {self._nodename} (master) to estimate the size of dataset at {dataset_path}: {str(e)}"
            )
            raise

        return int(val) * 1000 if val.isdigit() else sys.maxsize

    # Tha main method that copies the data into the Alluxio in-memory distributed cache cluster
    # This method assumes that all checks and validations are already done.

    @classmethod
    def get_capacity_bytes(cls):

        """This method returns the total capacity of in-memory Alluxio cache cluster"""

        # TODO: Use Api server and sidecar ..
        try:
            val = (
                sp.run(
                    (
                        "kubectl exec -it alluxio-master-0 -c alluxio-master -n "
                        + NAMESPACE
                        + " -- alluxio fs get_capacity_bytes"
                    ).split(),
                    stdout=sp.PIPE,
                )
                .stdout.decode("utf-8")
                .split()[-1]
            )

        except sp.CalledProcessError as e:
            log.exception(
                f"ERROR: Unable to connect to alluxio-master to estimate the total cache capacity before copying dataset: {str(e)}"
            )
            raise

        return int(val) if val.isdigit() else -sys.maxsize

    # This method returns the total size of used bytes of in-memory Alluxio cache cluster

    @classmethod
    def get_used_bytes(cls):

        """This method returns the total size of used bytes of in-memory Alluxio cache cluster"""

        # TODO: Use Api server and sidecar ..
        try:
            val = (
                sp.run(
                    (
                        "kubectl exec -it alluxio-master-0 -c alluxio-master -n "
                        + NAMESPACE
                        + " -- alluxio fs get_used_bytes"
                    ).split(),
                    stdout=sp.PIPE,
                )
                .stdout.decode("utf-8")
                .split()[-1]
            )

        except sp.CalledProcessError as e:
            log.exception(
                f"ERROR: Unable to connect to alluxio-master to estimate the size of cache available before copying dataset at {dataset_path}: {str(e)}"
            )
            raise

        return int(val) if val.isdigit() else sys.maxsize


# Manager class to handle copying and fairly distributing the data across all workers roughly equally


class WorkerNodeTaskExecutor(ExecutorServiceABC):

    """
    Manager class to handle copying the data onto the provided workers
    """

    def __init__(self, node_name):
        self._nodename = node_name

    # This method measures the returns the total space required to store the given dataset.

    def get_space_needed_for_dataset_in_bytes(self, dataset_path):

        """This method measures the returns the total space required to store the given dataset."""

        try:
            val = (
                sp.run(
                    ("ssh nikunj@" + self._nodename + " du -s " + dataset_path).split(),
                    stdout=sp.PIPE,
                )
                .stdout.decode("utf-8")
                .split()[0]
            )

        except sp.CalledProcessError as e:
            log.exception(
                f"ERROR: Unable to connect to {self._nodename} (worker) to estimate the size of dataset at {dataset_path}: {str(e)}"
            )
            raise

        # raise sp.CalledProcessError if data is not present or
        # there are other errors connectinng to the worker.

        return int(val) * 1000 if val.isdigit() else sys.maxsize

    # This method returns the total capacity and size of used bytes
    # of in-memory Alluxio cache cluster

    def get_capacity_and_used_bytes_onworker(self):

        """
        This method returns the total capacity and size of used bytes
        of in-memory Alluxio cache cluster
        """

        try:
            # Get Alluxio worker running on this node
            # TODO: Use Api server and sidecar ..
            # Tmp            out1 = sp.run(
            # Tmp            ("kubectl get pods --no-headers -o custom-columns=\":metadata.name,:status.podIP\"  --field-selector spec.nodeName=" + self._nodemae).split(),
            # Tmp            stdout=sp.PIPE, stderr=sp.PIPE)
            # Tmp
            # Tmp            out2 = sp.run(
            # Tmp            "grep alluxio-worker".split(), stdin=out1.stdout, stdout=sp.PIPE)
            # Tmp
            # Tmp            out, err = out2.communicate()
            # Tmp
            # Tmp            worker_pod, worker_ip = out[0:1]
            # Tmp
            # Tmp            log.info(f"Calculating space allocated for Alluxio worker {out.split[0]} running on node {self._nodename}...")
            # Tmp
            # Tmp            # Issue GET query using Python to gather capacity info
            # Tmp            command = f"kubectl exec -it {worker_pod} -c alluxio-worker -- \"echo -e \\"import requests; print(requests.get('http://localhost:30000/api/v1/worker/info').json())\\"\""
            # Tmp
            # Tmp            out1 = sp.run(command.split(), stdout=sp.PIPE).stdout
            # Tmp
            # Tmp            # Process response
            # Tmp            capacity, used = json.load(out1)
            # Tmp

            TMP_num_workers = 2

            capacity = MasterNodeTaskExecutor.get_capacity_bytes() / TMP_num_workers
            used = MasterNodeTaskExecutor.get_used_bytes() / TMP_num_workers

            log.info(
                f"Returning estimated total capacity = {capacity/1e6} MB and used = {used/1e6} MB for worker {self._nodename}"
            )
            return capacity, used

        except Exception as e:
            log.exception(str(e))

        return -1, -1


# Alluxio Cache Manager
# The class responsible for copying data into Alluxio when a deployment/job/pod etc
# have the `cacheDataset` annotation set to "yes" with quotes.


class AlluxioCacheManager(DatasetCacheManager):

    """
    Alluxio Cache Manager
    The class responsible for copying data into Alluxio when a deployment/job/pod etc
    have the `cacheDataset` annotation set to "yes" with quotes.
    """

    def __init__(self):

        self._print_banner()

        #        cfg = c.Configuration()
        #        cfg.api_key["authorization"] = os.getenv("TOKEN")
        #        cfg.host = "http://localhost"
        #        cfg.port = K8S_API_PORT # this line by nikunj
        #        self._cfg = cfg

        self._deploy_name = ""
        self._output_filename = ""

        self._map_k8s_cluster_nodes_alluxio_workers()  # TODO

    def _print_banner(self):

        log.info("-" * 80)
        log.info("Futurewei Technologies, Inc.")
        log.info("Copyright: All rights reserved (2022+)")
        log.info("Author: Nikunj J Parekh")
        log.info("Email: nparekh@futurewei.com")
        log.info("Alluxio Dataset Cache Operator Service started")
        log.info("-" * 80 + "\n")

    def _map_k8s_cluster_nodes_alluxio_workers(self):
        pass

    # Todo list for 9/20/2022:
    #  - CRD = AlnairDatacache will always stay deployed. Job is to create AlnairDatacache "CR" per below algorithm:
    #
    #  - fix functions that are "pass"
    #  - remove loop over RESOURCE_URI_MAP and use AlnairDatacache deployment type
    #  - If a new pod has cacheDataset="yes", absorb / read and delete that deployment/pod.
    #    Operator should create new "CR" of type AlnairDatacache after deleting ^
    #    This CR "copy" (python code of Operator) big data if there's space
    #    This CR "modifies" volumes and volumeMounts and keeps all other aspects of original deployment/job same.
    #    Deploys new item
    #
    #    Next, check perf: read many images with timestamps with and without alluxio
    #
    #    Next, test cache locality
    #
    #    Check all TODO

    # TODO:
    # import kubernetes as k
    # core = k.client.CoreV1Api()
    # res = core.read_namespace()
    # res = core.read_namespace('default')
    # d = res.to_dict()
    # import yaml
    # yaml.safe_dump(d)
    # print(yaml.safe_dump(d))

    # from kubernetes import client
    # config = client.Configuration()
    # apiclient = client.ApiClient(config)
    # v1 = client.CoreV1Api(apiclient)
    #    for pod in v1.list_namespaced_pod(namespace='default',watch=False):
    #        print(f"{pod.metadata.name}, in {pod.metadata.namespace}, IP {pod.status.pod_ip}")
    #        for pod in v1.list_namespaced_pod(namespace='default',watch=False).items:
    #            print(f"{pod.metadata.name}, in {pod.metadata.namespace}, IP {pod.status.pod_ip}")
    #            import readline
    # for i in range(readline.get_current_history_length()):
    #    print(readline.get_history_item(i+1))

    # TO GET TOKEN:
    # export API_TOKEN=$(kubectl get secret default-token-znbht -o jsonpath='{.data.token}' | base64 --decode)
    # TO USE TOKEN:
    # curl -X GET localhost:K8S_API_PORT/api --header "Authorization: Bearer $API_TOKEN" --insecure

    # This method returns the total capacity of in-memory Alluxio cache cluster

    def copy_data_to_datasetcache(self, beyond_mount_point, dataset_path, resource_ver):

        """Tha main method that copies the data into the Alluxio in-memory distributed cache cluster
        This method assumes that all checks and validations are already done."""

        # TODO: Use Api server and sidecar ..
        cp_cmd = (
            "kubectl exec -it alluxio-master-0 -c alluxio-master -n "
            + NAMESPACE
            + " -- alluxio fs copyFromLocal --thread 64 "
            + "/journal/datasets/"
            + beyond_mount_point  # /journal/datasets/<host_path without /mnt/fuse: sort of experiment path>
            + " "
            + dataset_path
        )

        cmd_stdout = ""
        try:
            out = sp.run(
                cp_cmd.split(),
                stdout=sp.PIPE,
                stderr=sp.PIPE,
            )
            cmd_stdout, cmd_stderr = out.stdout, out.stderr

            log.debug(f"\tCOMMAND = {cp_cmd}")
            log.debug(f"\tOUTPUT of the hydration command: {cmd_stdout}")
            log.debug(f"\tSTDERR of the hydration command: {cmd_stderr}")
            log.info(
                f"Data hydration operation completed for resource {self._deploy_name} (revision: {resource_ver} handled.)"
            )

        except Exception as e:  # Also captures CalledProcessError when actual cmd fails
            log.error(f"COMMAND = {cp_cmd}")
            log.error(f"OUTPUT of the hydration command: {cmd_stdout}")
            log.error(f"\tSTDERR of the hydration command: {cmd_stderr}")
            log.exception(str(e))
            log.exception("Data hydration operation ignored")

            return False

        return True

    def _delete_original_resource(self, resource):

        # TODO: Use Api server and sidecar ..

        log.info(
            f"Now deleting {resource} {self._deploy_name} and creating equivalent AlnairDatacache deployment"
        )
        log.info(
            "This will copy your dataset and redeploy your pod's internal container / command..."
        )

        del_cmd = (
            f"kubectl delete {resource} {self._deploy_name} --wait --now -n "
            + NAMESPACE
        )

        log.info(f"DEBUG: Delete cmd: {del_cmd}")

        try:
            sp.run(del_cmd, check=True, shell=True, timeout=None)

        except sp.CalledProcessError as e:
            log.exception(
                f"ERROR: Deployment failed for AlnairDatacache resource: {str(e)}"
            )
            raise

    def _check_caching_required(self, spec):

        need_alluxio = False

        volumes = spec.get("volumes", [])

        for v in volumes:
            need_alluxio = (
                True
                if "hostPath" in v
                else log.debug(
                    f"No hostPath for volume spec {v}.. checking next volumeMount.."
                )
            )
            if need_alluxio:
                return True

        if not need_alluxio:
            log.warning(
                "Did not require caching of Dataset for this deployment (perhaps, there's no hostPath and volumeMount)."
            )

        return False

    def _create_alnair_datacache_custom_resource(self, resource):
        # TODO: Separate out all kubernetes library module work into other python module and/or class

        log.info(
            f"Creating a new AlnairDatacache deployment for {resource} {self._deploy_name}"
        )

        create_cr_cmd = f"/create-and-deploy-alnair-datacache-resource {resource} {self._deploy_name} {CR_EXTENSION}"

        try:
            sp.run(create_cr_cmd, check=True, shell=True, timeout=None)

        except sp.CalledProcessError as e:
            log.exception(
                f"ERROR: Deployment creation program failed to create AlnairDatacache deployment: {str(e)}"
            )
            raise

    # DOES NOT WORK AS ADVERTISED in k8s 1.21+
    # api_inst.create_namespaced_custom_object(
    # group="centaurusinfra.com", version="v1",
    # namespace=NAMESPACE, plural="datacaches", body=data)

    def _perform_data_caching(self, spec):

        volumes = spec.get("volumes", [])

        for v in volumes:

            # Build from and to paths for copying data

            if "hostPath" not in v:
                log.debug(
                    f"No hostPath for volume spec {v}.. checking next volumeMount.."
                )
                continue

            vname, host_path = v["name"], v["hostPath"]["path"]

            # chomp trailing /
            if host_path[-1] == "/":
                host_path = host_path[:-1]

            # host_path is expected to be already in /mnt/fuse or /mnt/fuse3 location,
            # as such, in the format of /mnt/fuse3/datasets/experiment_name/data/ etc.
            # we need to extract the experiment name, we believe it'd be the leaf level dir name.

            # TODO: SUPPORT  host_path is /nfs ...

            experiment_leaf_dir = host_path.split("/")[-1]  # experiment name

            NEW_dataset_path = os.path.join(ALLUXIO_BASE_PATH, experiment_leaf_dir)

            # Get current size and capacity of Data Cache

            if scheduled_nodeName:
                capacity, used = worker_executor.get_capacity_and_used_bytes_onworker()
                new_data_size = worker_executor.get_space_needed_for_dataset_in_bytes(
                    host_path
                )

            else:
                capacity = MasterNodeTaskExecutor.get_capacity_bytes()
                used = MasterNodeTaskExecutor.get_used_bytes()
                new_data_size = (
                    MasterNodeTaskExecutor.get_space_needed_for_dataset_in_bytes(
                        host_path
                    )
                )

            log.debug(f"Total size of new data: {new_data_size / 1e6} MB")
            log.debug(f"Total cache capacity: {capacity / 1e6} MB")
            log.debug(
                f"Total space required (10% margin): {1.1 * new_data_size / 1e6} MB"
            )

            # Leave 10% margin of empty cache, for now - TODO: either configurable or revise value later

            if capacity - used >= 1.1 * new_data_size:

                # TODO: Find a way to "claim" or block before copying
                log.debug(
                    "Data Cache has sufficient capacity to copy dataset. Starting data hydration operation"
                )
                log.debug(
                    f"Caching data from {host_path} to {NEW_dataset_path} inside Data Cache..."
                )

                # /mnt/fuse gets removed, gets rest of path
                beyond_mount_point = "/".join(host_path.split("/")[4:])

                success = self.copy_data_to_datasetcache(
                    beyond_mount_point,
                    NEW_dataset_path,
                    resource_ver,
                )

                if success:
                    log.info(
                        f"Updated free cache capacity: {int((capacity - used)/1e6)} MB\n"
                    )

            else:
                log.warning(
                    "Data Caching request has been ignored due to lack of available cache capacity"
                )

    def _deploy_alnair_datacache_custom_resource(self, resource):

        log.info(f"Deploying new custom resource {self._deploy_name}-{CR_EXTENSION}...")

        deploy_cmd = f"kubectl create -f {self._output_filename} --validate=false"

        try:
            sp.run(deploy_cmd, check=True, shell=True, timeout=None)

        except sp.CalledProcessError as e:
            log.exception(
                f"ERROR: Deployment failed for AlnairDatacache resource: {str(e)}"
            )
            raise

    # This method processes the fields of the newly deploy-ing resource object to check for
    # the "cacheDataset" attribute. If this attribute is found, the methods triggers
    # copying of the dataset available on hostPath location, into the Alluxio in-memory cache.

    def process_new_resource_object(self, resource, obj, resource_ver):

        """
        This method processes the fields of the newly deploy-ing resource object to check for
        the "cacheDataset" attribute. If this attribute is found, the methods triggers
        copying of the dataset available on hostPath location, into the Alluxio in-memory cache.
        a hostPath can be a local dir, or an NFS volume, or a S3 mount or any other mount point, and
        no change is required to this code.
        Necessary conditions for data caching to trigger are:
        1) annotation cacheDataset: "yes" (and not "no" etc), must be present in metadata spec.
        2) a volume.path and a volumeMount.mountPath must be declared with valid .path and .mountPath.

        Furthermore, if a nodeName or a pod affinity/anti-affinity or a pod topology spec is present
        in the event, it'd be considered an affinity spec and all of the data will be copied over into
        that worker.
        """

        event_type = obj.get("type")

        if not event_type:
            return False, None

        if event_type not in ("MODIFIED", "ADDED"):  # "DELETED" ignored
            return False, None

        metadata = obj.get("object").get("metadata")

        if not metadata or "deletionTimestamp" in metadata:
            return False, None

        success = False

        try:

            self._deploy_name = metadata.get("name")

            self._output_filename = (
                f"datacache-custom-resource-{self._deploy_name}-{resource}.yml"
            )

            annotations = metadata.get("annotations")
            do_cache_dataset = (
                annotations.get("cacheDataset", "no").casefold() == "yes"
                if annotations
                else False
            )

            spec = (
                obj.get("object", {})
                .get("spec", {})
                .get("template", {})
                .get("spec", {})
            )

            if not spec or not self._check_caching_required(spec):
                return False

            scheduled_nodeName = spec.get("nodeName")

            if scheduled_nodeName:
                log.info(
                    f"Node affinity is declared for deployment {self._deploy_name} with node {scheduled_nodeName}"
                )

            worker_executor = (
                WorkerNodeTaskExecutor(scheduled_nodeName)
                if scheduled_nodeName
                else None
            )

            # What is affnity specified differently -- TODO HANDLE
            ## Also, what if no affinity specified, then query and use the worker the pod is about to deploy to ####

            if do_cache_dataset:

                log.info(
                    f"Dataset Caching is requested by {resource}: {self._deploy_name} for resource update revision: {resource_ver}\n"
                )

                # Since this new deployment has cacheDataset="yes", absorb / read and delete that deployment/pod.

                caching_needed = self._create_alnair_datacache_custom_resource(resource)

                if caching_needed:

                    self._delete_original_resource(resource)

                    self._perform_data_caching(spec)

                    self._deploy_alnair_datacache_custom_resource(resource)

                    log.info(
                        f"Completed custom resource creation for your deployment {self._deploy_name}"
                    )

        except Exception as e:
            log.exception(
                "No Data hydration is possible in this deployment or pod or job"
            )

        return success

    def stop(self):
        # get all active child processes
        children = active_children()

        print(f"Terminating any active Children: {len(children)}")
        for c in children:
            c.terminate()  # Kill gracefully, keep children

    def abort(self):
        # get all active child processes
        children = active_children()

        print(f"Abort any active Children: {len(children)}")
        for c in children:
            c.kill()  # Kill immediately, kill any children as well

    # This is the main function that watches the Kubernetes API for changes as follows:
    # This method implements the main event loop to frequently check the kubernetes cluster for
    # any new deployments. If a new deployment or pod or job is detected, the method triggers
    # the process to check for need of Dataset Caching and copies the dataset into the Alluxio cache
    # if doing so is possible. """

    def event_loop(self):
        # Do not use kubernetes library module, its still flaky. sp is ok

        """This method implements the main event loop to frequently check the kubernetes cluster for
        any new deployments. If a new deployment or pod or job is detected, the method triggers
        the process to check for need of Dataset Caching and copies the dataset into the Alluxio cache
        if doing so is possible."""

        # We have to do this because resourceVersion and resourceVersionMatch features in K8s do not work as advertised

        resource_vers = set()
        # TODO: Store and get it from ConfigMap

        log.info(
            "Checking for newly deploying resources and resource updates that need Data Cache to be hydrated...\n"
        )

        while True:

            responses = {}

            # This is to avoid consecutive and quick duplicate update events for single change in a resource (because K8s isn't perfect)!

            for resource in RESOURCE_URI_MAP:

                url = f"{K8S_API_BASE_URL}/{RESOURCE_URI_MAP[resource]}/{resource}?watch=true"

                # Request the API endpoint and keep conenction open

                try:
                    responses[resource] = requests.get(url, stream=True)
                except requests.exceptions.HTTPError as e:
                    log.error(
                        f"An unrecognized response while querying {resource} is ignored: {str(e)}, {type(e)}"
                    )

            for resource in responses:
                try:
                    response = responses[resource]

                    if response.ok:
                        for line in response.iter_lines():
                            obj = json.loads(line)

                            new_version = (
                                obj.get("object").get("metadata").get("resourceVersion")
                            )

                            if new_version not in resource_vers:

                                log.debug(f"{resource_vers}")

                                success = self.process_new_resource_object(
                                    resource, obj, new_version
                                )

                                if success:

                                    if len(resource_vers) > 10:
                                        resource_vers.clear()

                                    resource_vers.add(new_version)
                                    break

                except DatasetCacheError as e:
                    log.error(
                        f"Unrecognized deployment object '{response}' is ignored: {str(e)}, {type(e)}"
                    )

            time.sleep(2)


def proxy_task():
    # while True:
    log.info(f"Starting Kubernetes Proxy on port {K8S_API_PORT}...")
    log.debug(f"Command: kubectl proxy --port={K8S_API_PORT}")

    os.system(f"kubectl proxy --port={K8S_API_PORT} 2>&1 > /dev/null")


try:
    # Instantiate the AlluxioCacheManager
    cache_manager = AlluxioCacheManager()

    # Run Kubernetes Proxy on port K8S_API_PORT
    daemon = threading.Thread(daemon=True, target=proxy_task, name="K8s_Proxy")
    daemon.start()

    time.sleep(1)

    # Start the event loop
    cache_manager.event_loop()

except Exception as e:
    log.exception(f"ERROR: {str(e)}")
    cache_manager.abort()

log.info("Stopping Cache Manager...")
cache_manager.stop()

log.info("Stopping Kubernetes Proxy...")
daemon.join()
