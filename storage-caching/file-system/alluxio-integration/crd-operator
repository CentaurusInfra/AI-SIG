#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Futurewei Technologies.
# Copyright: All rights reserved (2022+)
# Author: Nikunj J Parekh
# Email: nparekh@futurewei.com

import json
import logging
import os

# import kubernetes.client as c
import requests
import subprocess as sp
import sys
import time
import threading

from cache_manager import CacheManagerABC
from cache_manager import ExecutorServiceABC
from multiprocessing import active_children

# Declare const strings
NAMESPACE = os.getenv("res_namespace", "default")
ALLUXIO_BASE_PATH = "/futurewei-data/datasets"
CR_EXTENSION = "autogenerated-cr"
K8S_API_PORT = 8002
K8S_API_BASE_URL = f"http://127.0.0.1:{K8S_API_PORT}"

# K8S_PROXY_SSH_IP = "10.145.83.40"
# K8S_PROXY_SSH_IP = os.getenv("alluxio_master_host", "")

PODS_URI = f"api/v1/namespaces/{NAMESPACE}"
DEPLOYMENTS_URI = f"apis/apps/v1/namespaces/{NAMESPACE}"
JOBS_URI = f"apis/batch/v1/namespaces/{NAMESPACE}"
RESOURCE_URI_MAP = dict(deployments=DEPLOYMENTS_URI, pods=PODS_URI, jobs=JOBS_URI)


# Setup logging
log = logging.getLogger(__name__)
logging.basicConfig(
    format=" [%(asctime)s] | %(levelname)s\t|  %(message)s", level=logging.INFO
)


def warn(*foo):
    log.warning(*foo)


class DatasetCacheManager(CacheManagerABC):
    pass


# Exception handlers
class DatasetCacheError(Exception):
    pass


# Manager class to handle copying the data on to a single provided worker


class MasterNodeTaskExecutor(ExecutorServiceABC):

    """
    Manager Class to handle copying and fairly distributing the data across all workers roughly equally
    """

    @classmethod
    def get_space_needed_for_dataset_in_bytes(cls, dataset_path):

        """This method measures the returns the total space required to store the given dataset."""

        # Get the first worker node, always

        # TODO: Use Api server and sidecar ..
        host_list_cmd = "kubectl get nodes -o jsonpath='{$.items[*].status.addresses[?(@.type==\"InternalIP\")].address}'"

        host_names = sp.run(
            host_list_cmd.split(), stdout=sp.PIPE, check=True
        ).stdout.decode("utf-8")

        # Arbitrarily picking first worker node, and assuming its the second row in response to kubectl get nodes command
        host_name = host_names.split()[1]

        if not host_name:
            raise DatasetCacheError(
                "Dataset Cache operator failed to get the IP address of the first worker to access dataset"
            )

        try:
            val = (
                sp.run(
                    ("ssh nikunj@" + host_name + " du -s " + dataset_path).split(),
                    stdout=sp.PIPE,
                    check=True,
                )
                .stdout.decode("utf-8")
                .split()[0]
            )

        except sp.CalledProcessError as e:
            log.error(
                f"Unable to connect to Data Cache Master to estimate the size of dataset at {dataset_path}:"
            )
            log.error(str(e))
            raise

        return int(val) * 1000 if val.isdigit() else sys.maxsize

    # Tha main method that copies the data into the Alluxio in-memory distributed cache cluster
    # This method assumes that all checks and validations are already done.

    @classmethod
    def get_capacity_bytes(cls):

        """This method returns the total capacity of in-memory Alluxio cache cluster"""

        # TODO: Use Api server and sidecar ..
        try:
            val = (
                sp.run(
                    (
                        "kubectl exec -it alluxio-master-0 -c alluxio-master -n "
                        + NAMESPACE
                        + " -- alluxio fs getCapacityBytes"
                    ).split(),
                    stdout=sp.PIPE,
                    check=True,
                )
                .stdout.decode("utf-8")
                .split()[-1]
            )

        except sp.CalledProcessError as e:
            log.error(
                f"Unable to connect to Data Cache Master to estimate the total cache capacity before copying dataset:"
            )
            log.error(str(e))
            raise

        return int(val) if val.isdigit() else -sys.maxsize

    # This method returns the total size of used bytes of in-memory Alluxio cache cluster

    @classmethod
    def get_used_bytes(cls):

        """This method returns the total size of used bytes of in-memory Alluxio cache cluster"""

        # TODO: Use Api server and sidecar ..
        try:
            val = (
                sp.run(
                    (
                        "kubectl exec -it alluxio-master-0 -c alluxio-master -n "
                        + NAMESPACE
                        + " -- alluxio fs getUsedBytes"
                    ).split(),
                    stdout=sp.PIPE,
                    check=True,
                )
                .stdout.decode("utf-8")
                .split()[-1]
            )

        except sp.CalledProcessError as e:
            log.error(
                f"Unable to connect to Data Cache Master to estimate the size of cache available before copying dataset:"
            )
            log.error(str(e))
            raise

        return int(val) if val.isdigit() else sys.maxsize


# Manager class to handle copying and fairly distributing the data across all workers roughly equally


class WorkerNodeTaskExecutor(ExecutorServiceABC):

    """
    Manager class to handle copying the data onto the provided workers
    """

    def __init__(self, node_name):
        self._nodename = node_name

    # This method measures the returns the total space required to store the given dataset.

    def get_space_needed_for_dataset_in_bytes(self, dataset_path):

        """This method measures the returns the total space required to store the given dataset."""

        try:
            val = (
                sp.run(
                    ("ssh nikunj@" + self._nodename + " du -s " + dataset_path).split(),
                    stdout=sp.PIPE,
                    check=True,
                )
                .stdout.decode("utf-8")
                .split()[0]
            )

        except sp.CalledProcessError as e:
            log.error(
                f"Unable to connect to Data Cache worker {self._nodename} to estimate the size of cache available before copying dataset:"
            )
            log.error(str(e))
            raise

        # raise sp.CalledProcessError if data is not present or
        # there are other errors connectinng to the worker.

        return int(val) * 1000 if val.isdigit() else sys.maxsize

    # This method returns the total capacity and size of used bytes
    # of in-memory Alluxio cache cluster

    def get_capacity_and_used_bytes_onworker(self):

        """
        This method returns the total capacity and size of used bytes
        of in-memory Alluxio cache cluster
        """

        try:
            # Get Alluxio worker running on this node
            # TODO: Use Api server and sidecar ..
            # Tmp            out1 = sp.run(
            # Tmp            ("kubectl get pods --no-headers -o custom-columns=\":metadata.name,:status.podIP\"  --field-selector spec.nodeName=" + self._nodemae).split(),
            # Tmp            stdout=sp.PIPE, stderr=sp.PIPE)
            # Tmp
            # Tmp            out2 = sp.run(
            # Tmp            "grep alluxio-worker".split(), stdin=out1.stdout, stdout=sp.PIPE)
            # Tmp
            # Tmp            out, err = out2.communicate()
            # Tmp
            # Tmp            worker_pod, worker_ip = out[0:1]
            # Tmp
            # Tmp            log.info(f"Calculating space allocated for Alluxio worker {out.split[0]} running on node {self._nodename}...")
            # Tmp
            # Tmp            # Issue GET query using Python to gather capacity info
            # Tmp            command = f"kubectl exec -it {worker_pod} -c alluxio-worker -- \"echo -e \\"import requests; print(requests.get('http://localhost:30000/api/v1/worker/info').json())\\"\""
            # Tmp
            # Tmp            out1 = sp.run(command.split(), stdout=sp.PIPE).stdout
            # Tmp
            # Tmp            # Process response
            # Tmp            capacity, used = json.load(out1)
            # Tmp

            TMP_num_workers = 2

            capacity = MasterNodeTaskExecutor.get_capacity_bytes() / TMP_num_workers
            used = MasterNodeTaskExecutor.get_used_bytes() / TMP_num_workers

            log.info(
                f"Returning estimated total capacity = {capacity/1e6} MB and used = {used/1e6} MB for worker {self._nodename}"
            )
            return capacity, used

        except Exception as e:
            log.error("Unable to get total capacity and used capacity on worker node:")
            log.error(f"  {str(e)}")

        return -1, -1


# Alluxio Cache Manager
# The class responsible for copying data into Alluxio when a deployment/job/pod etc
# have the `cacheDataset` annotation set to "yes" with quotes.


class AlluxioCacheManager(DatasetCacheManager):

    """
    Alluxio Cache Manager
    The class responsible for copying data into Alluxio when a deployment/job/pod etc
    have the `cacheDataset` annotation set to "yes" with quotes.
    """

    def __init__(self):

        self._print_banner()

        #        cfg = c.Configuration()
        #        cfg.api_key["authorization"] = os.getenv("TOKEN")
        #        cfg.host = "http://localhost"
        #        cfg.port = K8S_API_PORT # this line by nikunj
        #        self._cfg = cfg

        self._deploy_name = ""
        self._output_filename = ""

        self._map_k8s_cluster_nodes_alluxio_workers()  # TODO

        self._daemons = [None] * 3

    def _print_banner(self):

        log.info("-" * 50)
        log.info("Futurewei Technologies, Inc.")
        log.info("Copyright: All rights reserved (2022+)")
        log.info("Author: Nikunj J Parekh")
        log.info("Email: nparekh@futurewei.com")
        log.info("Dataset Caching Operator started")
        log.info("-" * 50 + "\n")

    def _map_k8s_cluster_nodes_alluxio_workers(self):
        pass

    # Todo list for 9/20/2022:
    #  - CRD = AlnairDatacache will always stay deployed. Job is to create AlnairDatacache "CR" per below algorithm:
    #
    #  - fix functions that are "pass"
    #  - remove loop over RESOURCE_URI_MAP and use AlnairDatacache deployment type
    #  - If a new pod has cacheDataset="yes", absorb / read and delete that deployment/pod.
    #    Operator should create new "CR" of type AlnairDatacache after deleting ^
    #    This CR "copy" (python code of Operator) big data if there's space
    #    This CR "modifies" volumes and volumeMounts and keeps all other aspects of original deployment/job same.
    #    Deploys new item
    #
    #    Next, check perf: read many images with timestamps with and without alluxio
    #
    #    Next, test cache locality
    #
    #    Check all TODO

    # TODO:
    # import kubernetes as k
    # core = k.client.CoreV1Api()
    # res = core.read_namespace()
    # res = core.read_namespace('default')
    # d = res.to_dict()
    # import yaml
    # yaml.safe_dump(d)
    # print(yaml.safe_dump(d))

    # from kubernetes import client
    # config = client.Configuration()
    # apiclient = client.ApiClient(config)
    # v1 = client.CoreV1Api(apiclient)
    #    for pod in v1.list_namespaced_pod(namespace='default',watch=False):
    #        print(f"{pod.metadata.name}, in {pod.metadata.namespace}, IP {pod.status.pod_ip}")
    #        for pod in v1.list_namespaced_pod(namespace='default',watch=False).items:
    #            print(f"{pod.metadata.name}, in {pod.metadata.namespace}, IP {pod.status.pod_ip}")
    #            import readline
    # for i in range(readline.get_current_history_length()):
    #    print(readline.get_history_item(i+1))

    # TO GET TOKEN:
    # export API_TOKEN=$(kubectl get secret default-token-znbht -o jsonpath='{.data.token}' | base64 --decode)
    # TO USE TOKEN:
    # curl -X GET localhost:K8S_API_PORT/api --header "Authorization: Bearer $API_TOKEN" --insecure

    # This method returns the total capacity of in-memory Alluxio cache cluster

    def copy_data_to_datasetcache(self, beyond_mount_point, dataset_path, resource_rev):

        """Tha main method that copies the data into the Alluxio in-memory distributed cache cluster
        This method assumes that all checks and validations are already done."""

        # TODO: Use Api server and sidecar ..
        cp_cmd = (
            "kubectl exec -it alluxio-master-0 -c alluxio-master -n "
            + NAMESPACE
            + " -- alluxio fs copyFromLocal --thread 64 "
            + "/journal/"
            + beyond_mount_point  # /journal/datasets/<host_path without /mnt/fuse: sort of experiment path>
            + " "
            + dataset_path
        )

        out = None
        try:
            out = sp.run(
                cp_cmd.split(), stdout=sp.PIPE, stderr=sp.PIPE, check=True, timeout=None
            )

            log.debug(f"\tCOMMAND = {cp_cmd}")
            log.debug(f"OUTPUT of the hydration command: {out.stdout}")
            log.debug(f"STDERR of the hydration command: {out.stderr}")
            log.info(
                f"Data hydration completed for {self._deploy_name}, mount {beyond_mount_point}, path {dataset_path}, rev: {resource_rev})"
            )

        except Exception as e:  # Also captures CalledProcessError when actual cmd fails
            warn(
                f"Data hydration request may have been already satisfied. Check if data is already cached by browsing to the dashboard of Dataset Cache master."
            )
            # Suppress the unnecessary error when data is already cached (exit code 255 for copyFromLocal)
            # log.error(f"  {str(e)}")

            return False

        return True

    def _delete_original_resource(self, resource):

        # TODO: Use Api server and sidecar ..

        log.debug(
            f"Now deleting {resource} {self._deploy_name} and creating equivalent AlnairDatacache deployment"
        )

        del_cmd = (
            # f"kubectl delete {resource} {self._deploy_name} --wait=true --now=true --timeout=10s -n "
            f"kubectl delete {resource} {self._deploy_name} --wait=false --now=true -n "
            + NAMESPACE
        )

        log.debug(f"DEBUG: Delete cmd: {del_cmd}")

        try:
            sp.run(del_cmd.split(), check=True, timeout=60)

        except sp.CalledProcessError as e:
            log.error(f"Failed to delete original resource {self._deploy_name}:")
            log.error(str(e))

    def _check_caching_required(self, spec):

        need_alluxio = False

        volumes = spec.get("volumes", [])

        for v in volumes:
            need_alluxio = (
                True
                if "hostPath" in v
                else log.debug(
                    f"No hostPath for volume spec {v} in {self._deploy_name}"
                )
            )
            if need_alluxio:
                return True

        if "alnair-datacache-operator" not in self._deploy_name:
            log.debug(
                f"Dataset hydration is not required for resource {self._deploy_name}"
            )

        return False

    def _create_alnair_datacache_custom_resource(self, resource):
        # TODO: Separate out all kubernetes library module work into other python module and/or class

        create_cr_cmd = f"/create-alnair-datacache-resource {resource} {self._deploy_name} {CR_EXTENSION}"

        try:
            return (
                sp.run(create_cr_cmd.split(), stderr=None, timeout=60).returncode == 0
            )

        except Exception as e:
            log.error(
                f"Deployment creation program failed to create AlnairDatacache deployment:"
            )
            log.error(str(e))

        return False

    # DOES NOT WORK AS ADVERTISED in k8s 1.21+
    # api_inst.create_namespaced_custom_object(
    # group="centaurusinfra.com", version="v1",
    # namespace=NAMESPACE, plural="datacaches", body=data)

    def _perform_data_caching(
        self, spec, scheduled_nodeName, worker_executor, resource_rev
    ):

        volumes = spec.get("volumes", [])

        for v in volumes:

            # Build from and to paths for copying data

            if "hostPath" not in v:
                log.debug(
                    f"No hostPath for volume spec {v}.. checking next volumeMount.."
                )
                continue

            vname, host_path = v["name"], v["hostPath"]["path"]

            # chomp trailing /
            if host_path[-1] == "/":
                host_path = host_path[:-1]

            # host_path is expected to be already in /mnt/fuse or /mnt/fuse3 location,
            # as such, in the format of /mnt/fuse3/alluxio-journal/experiment_name/data/ etc.
            # we need to extract the experiment name, we believe it'd be the leaf level dir name.

            # TODO: SUPPORT  host_path is /nfs

            experiment_leaf_dir = host_path.split("/")[-1]  # experiment name

            # NEW_dataset_path = os.path.join(ALLUXIO_BASE_PATH, vname, experiment_leaf_dir)
            NEW_dataset_path = os.path.join(ALLUXIO_BASE_PATH, experiment_leaf_dir)

            # Get current size and capacity of Dataset Cache

            if scheduled_nodeName:
                log.info(
                    f"Node affinity is identified for in-progress deployment {self._deploy_name} with node {scheduled_nodeName}"
                )
                capacity, used = worker_executor.get_capacity_and_used_bytes_onworker()
                new_data_size = worker_executor.get_space_needed_for_dataset_in_bytes(
                    host_path
                    # Cached-data is mounted, only in workers, at path after caching: /mnt/alluxio-fuse/futurewei-data/datasets
                    # but only after data copy happens using copyFromLocal
                    # hostPath: /mnt/fuse3/alluxio-journal/demo-dataset
                    # TODO TODO-------------asdasdasdkajsdkljlsdjas'daj'DJ'fHK
                )

            else:
                capacity = MasterNodeTaskExecutor.get_capacity_bytes()
                used = MasterNodeTaskExecutor.get_used_bytes()
                new_data_size = MasterNodeTaskExecutor.get_space_needed_for_dataset_in_bytes(
                    host_path
                )

            log.debug(f"Total size of new data: {int(new_data_size / 1e6)} MB")
            log.debug(f"Total cache available: {int((capacity - used)/ 1e6)} MB")
            log.debug(
                f"Total space required (10% margin): {int(1.1 * new_data_size / 1e6)} MB"
            )

            # Leave 10% margin of empty cache, for now - TODO: either configurable or revise value later

            if capacity - used >= 1.1 * new_data_size:

                # TODO: Find a way to "claim" or block before copying
                log.debug(
                    "Dataset Cache has sufficient capacity to copy dataset. Starting data hydration"
                )
                log.info(
                    f"Hydrating cache with data from {host_path} to {NEW_dataset_path}"
                )

                # /mnt/fuse gets removed, gets rest of path
                beyond_mount_point = "/".join(host_path.split("/")[4:])

                success = self.copy_data_to_datasetcache(
                    beyond_mount_point, NEW_dataset_path, resource_rev
                )

                if success:
                    log.info(f"Total cache available: {int((capacity - used)/1e6)} MB")

            else:
                warn(
                    "Data hydration request is ignored due to lack of available cache capacity."
                )
                warn(f"  Cache capacity used:      {used}")
                warn(f"  Cache capacity available: {capacity-used}")
                warn(f"  Cache capacity required:  {1.1*new_data_size}")

    def _deploy_alnair_datacache_custom_resource(self, resource):

        log.info(
            f"Now deploying new custom resource {self._deploy_name}-{CR_EXTENSION}"
        )

        # Create / Replace the deployment
        # deploy_cmd = f"kubectl create -f {self._output_filename} --validate=false"
        deploy_cmd = (
            f"kubectl replace --force -f {self._output_filename} --validate=false"
        )

        # TODO: Check if its cumbersome to avoid --validate

        out = None
        try:
            out = sp.run(
                deploy_cmd.split(),
                check=True,
                stdout=sp.PIPE,
                stderr=sp.PIPE,
                timeout=60,
            )

            self._mark_done()

        except sp.CalledProcessError as e:
            warn("Deployment failed for AlnairDatacache resource:")
            warn(f"  Command: {deploy_cmd}")
            warn(
                "  Please check if the corresponding AlnairDatacache deployment is already running"
            )

    def _mark_done(self):
        try:
            mv_cmd = f"mv {self._output_filename} {self._output_filename}.done"
            log.debug(f"Moving the {self._output_filename} to .done..")
            sp.run(
                mv_cmd.split(),
                check=False,
                stdout=sp.DEVNULL,
                stderr=sp.DEVNULL,
                timeout=10,
            )
        except:
            pass

    # This method processes the fields of the newly deploy-ing resource object to check for
    # the "cacheDataset" attribute. If this attribute is found, the methods triggers
    # copying of the dataset available on hostPath location, into the Alluxio in-memory cache.

    def process_new_resource_object(self, resource, obj, resource_rev):

        """
        This method processes the fields of the newly deploy-ing resource object to check for
        the "cacheDataset" attribute. If this attribute is found, the methods triggers
        copying of the dataset available on hostPath location, into the Alluxio in-memory cache.
        a hostPath can be a local dir, or an NFS volume, or a S3 mount or any other mount point, and
        no change is required to this code.
        Necessary conditions for dataset caching to trigger are:
        1) annotation cacheDataset: "yes" (and not "no" etc), must be present in metadata spec.
        2) a volume.path and a volumeMount.mountPath must be declared with valid .path and .mountPath.

        Furthermore, if a nodeName or a pod affinity/anti-affinity or a pod topology spec is present
        in the event, it'd be considered an affinity spec and all of the data will be copied over into
        that worker.
        """

        event_type = obj.get("type")

        if not event_type:
            return False, None

        if event_type not in ("MODIFIED", "ADDED"):  # "DELETED" ignored
            return False, None

        success = False

        metadata = obj.get("object").get("metadata")

        if not metadata or "deletionTimestamp" in metadata:
            return False, None

        try:

            self._deploy_name = metadata.get("name")

            self._output_filename = (
                f"datacache-custom-resource-{self._deploy_name}-{resource}.yml"
            )

            annotations = metadata.get("annotations")
            do_cache_dataset = (
                annotations.get("cacheDataset", "no").casefold() == "yes"
                if annotations
                else False
            )

            if resource == "deployments":
                spec = (
                    obj.get("object", {})
                    .get("spec", {})
                    .get("template", {})
                    .get("spec", {})
                )
            elif resource == "pods":
                spec = obj.get("object", {}).get("spec", {})
            elif resource == "jobs":
                spec = obj.get("object", {}).get("spec", {})

            if not spec or not self._check_caching_required(spec):
                return False

            scheduled_nodeName = None  # spec.get("nodeName")

            worker_executor = (
                WorkerNodeTaskExecutor(scheduled_nodeName)
                if scheduled_nodeName
                else None
            )

            # What is affnity specified differently -- TODO HANDLE
            ## Also, what if no affinity specified, then query and use the worker the pod is about to deploy to ####

            if do_cache_dataset:

                # Since this new deployment has cacheDataset="yes", absorb / read and delete that deployment/pod.

                if self._create_alnair_datacache_custom_resource(resource):

                    log.info(
                        f"Dataset hydration requested by {resource}: {self._deploy_name} for event rev: {resource_rev}"
                    )
                    log.info(
                        f"Created new AlnairDatacache deployment for {resource} {self._deploy_name}"
                    )

                    self._delete_original_resource(resource)

                    self._perform_data_caching(
                        spec, scheduled_nodeName, worker_executor, resource_rev
                    )

                    self._deploy_alnair_datacache_custom_resource(resource)

                    log.info(
                        f"Completed custom resource creation for resource: {self._deploy_name}\n"
                    )
                    success = True

                else:
                    self._mark_done()

        except Exception as e:
            log.error(
                f"Data hydration is not possible for resource {self._deploy_name} due to errors:"
            )
            log.error(str(e))

        worker_executor = None

        return success

    def _stop_daemons(self):
        for d in self._daemons:
            d.join()
        self._daemons = []

    def stop(self):

        self._stop_daemons()

        # get all active child processes
        children = active_children()

        warn(f"Terminating any active Children: {len(children)}")
        for c in children:
            c.terminate()  # Kill gracefully, keep children

    def abort(self):

        self._stop_daemons()

        # get all active child processes
        children = active_children()

        warn(f"Abort any active Children: {len(children)}")
        for c in children:
            c.kill()  # Kill immediately, kill any children as well

    def start(self):

        log.debug("Inside top level start of CacheManager..")

        self._daemons = [
            threading.Thread(
                daemon=True,
                target=self.run,
                args=(resource,),
                name=f"{resource} processor",
            )
            for resource in RESOURCE_URI_MAP
        ]

        for d in self._daemons:
            d.start()

        # Main loop; does nothing
        while True:
            time.sleep(100)

    # This is the main function that watches the Kubernetes API for changes as follows:
    # This method implements the main event loop to frequently check the kubernetes cluster for
    # any new deployments. If a new deployment or pod or job is detected, the method triggers
    # the process to check for need of Dataset Caching and copies the dataset into the Alluxio cache
    # if doing so is possible. """

    def run(self, resource):
        # Do not use kubernetes library module, its still flaky. sp is ok

        """This method implements the main event loop to frequently check the kubernetes cluster for
        any new deployments. If a new deployment or pod or job is detected, the method triggers
        the process to check for need of Dataset Caching and copies the dataset into the Alluxio cache
        if doing so is possible."""

        # We have to do this because resourceVersion and resourceVersionMatch features in K8s do not work as advertised

        resource_vers = set()
        # TODO: Store and get it from ConfigMap

        log.info(
            f"Checking for new resources and updates to {resource} that need Dataset Cache to be hydrated..."
        )

        url = f"{K8S_API_BASE_URL}/{RESOURCE_URI_MAP[resource]}/{resource}?watch=true"

        while True:

            # Request the API endpoint and keep conenction open

            try:
                log.debug(f"\turl: {url}")
                response = requests.get(url, stream=True)
                log.debug("\tresponse recd")

            except requests.exceptions.HTTPError as e:
                warn(
                    f"An unrecognized response while querying {url} is ignored: {str(e)}, {type(e)}"
                )

            try:
                if response.ok:
                    for line in response.iter_lines():
                        obj = json.loads(line)

                        new_version = (
                            obj.get("object", {})
                            .get("metadata", {})
                            .get("resourceVersion", None)
                        )

                        if new_version and new_version not in resource_vers:

                            log.debug(f"{resource_vers}")

                            success = self.process_new_resource_object(
                                resource, obj, new_version
                            )

                            if success:

                                if len(resource_vers) > 10:
                                    resource_vers.clear()

                                resource_vers.add(new_version)
                else:
                    log.debug("response != ok")

            except DatasetCacheError as e:
                warn(
                    f"Unrecognized deployment object '{response}' is ignored: {str(e)}, {type(e)}"
                )

            time.sleep(2)


def proxy_task():

    # proxy_cmd = f"/deploy-proxy {K8S_PROXY_SSH_IP} {K8S_API_PORT} {ALLUXIO_BASE_PATH}"
    proxy_cmd = f"/deploy-proxy {K8S_API_PORT} {ALLUXIO_BASE_PATH}"

    out = None
    try:
        out = sp.run(
            proxy_cmd.split(), stdout=sp.PIPE, stderr=sp.PIPE, check=True, timeout=None
        )

    except sp.CalledProcessError as e:
        log.exception("Proxy deployment failed")
        raise


try:
    # Instantiate the AlluxioCacheManager
    cache_manager = AlluxioCacheManager()

    # Run Kubernetes Proxy on port K8S_API_PORT
    proxy_daemon = threading.Thread(daemon=True, target=proxy_task, name="K8s_Proxy")
    proxy_daemon.start()

    time.sleep(3)  # Let the proxy server start
    warn("Node affinity is ignored for now")

    # Start the event loop
    cache_manager.start()

    log.info("Stopping Cache Manager...")
    cache_manager.stop()

    log.info("Stopping Kubernetes Proxy...")

    proxy_daemon.join()
    if proxy_daemon.isAlive():
        raise SystemExit()

except Exception as e:
    log.exception(f"{str(e)}")
    cache_manager.abort()
    raise SystemExit()
