# Alnair Exporter

## Introduction
Alnair exporter is a Prometheus exporter with custom collectors to collector metrics cross multiple levels including cuda level, gpu process level, and python level(TBD). Through monitoring AI workloads at multiple level and fine-grained, we can better understand the workloads charactersitcs in terms of resource utilization. This also enables cross-layer correlation analysis to indentify performance bottlenecks. 

The design includes an exporter module, which instantiates a http server with Prometheus handle to post metrics under ```/metrics```, and register custom collectors. Currently three collectors are implemented, ands ix metrics with alnair prefix are posted. More collectors and metrics can be added easily. Each collector implements the collect and describe interface defined by Promtheus. Collect functions are called when a pull request is sent from Promtheus Servers. Metrics are either collected from an existing server through API calls, e.g., nvml or parsed from a stats file dynamically updated by other processes, e.g., ```/proc/stats``` for cpu time, and ```/var/lib/alnair/workspace/<alnairID>/metrics.log```, generated by [alnair intercept lib](https://github.com/CentaurusInfra/alnair/tree/main/alnair-device-plugin/intercept-lib).

The exporter will not report data proactively. Data pulling is triggered from Prometheus server side. The pulling/scraping frequency is currently set at 1 second in the Promtheus configuration file.

## Custom Collector

### 1.nvml collector
nvml collector use [Nvidia's nvml lib go binding](https://github.com/NVIDIA/go-nvml). Function ```nvml.DeviceGetProcessUtilization()``` is able to get us GPU process level utilization, which is important for GPU sharing scenario. Default Nvidia [DCGM exporter](https://github.com/NVIDIA/dcgm-exporter) can only report GPU utilization at card level. If multiple processes run on the same GPU, we cannot tell the utilization of a certain process/application. The return value of ```nvml.DeviceGetProcessUtilization()``` is a list of samples, each sample includes 6 elements, ```[pid, timestamp, sm util, mem util, dec util, enc util]```. nvml collector parse and report pid, sm util, and mem uitl.

### 2.cpu collector
cpu collector is simplely parsed ```/proc/stats/``` file. In file ```/proc/stats```, each line has 11 fields, ```cpu_id, user, nice,	system,	idle,	iowait,	irq,	softirq,	steal,	guest,	guest_nice```. User, system and idle time of each cpu core are extracted. CPU utilization can be calculated by the time differences of two samples. Process-level cpu utilization can be caluated through parsing ```/proc/<pid>/stats/```, which has not been implemented yet.

### 3.cuda collector
cuda collector is similar to cpu collector. It reports three metrics, ```kernel-cnt, mem-used, token-cnt```, by iterating all the folders under ```/var/lib/alnair/workspace/```, and parses the following two files. ```metrics.log``` file is generated by [Alnair intercept lib](https://github.com/CentaurusInfra/alnair/tree/main/alnair-device-plugin/intercept-lib). Metrics in metrics.log is updated every 0.01 second. The ```kernel-cnt``` is a counter which only goes up. The ```token-cnt``` is an internal value used in Alnair device plugin to limit the kernel launch frequency. Therefore, shared GPU compute resource can be guaranteed. The pod name and namespace stored in ```podInfo``` file together with ```pid``` are used as labels for the three metrics.

```metrics.log```
```
pid:78506
kernel-cnt:68584
mem-used:180227622
token-cnt:362676
```

```podInfo```
```
podName:sharing-pod-1
namespace:default
```
## Metrics
The metrics name posted to Prometheus are the followings.

**alnair_cpustat_time_user_hertz_total**

**alnair_cuda_available_token**

**alnair_cuda_launch_kernel_cnt**

**alnair_cuda_mem_used**

**alnair_gpu_mem_used**

**alnair_gpu_util**

## Installation 
Alnair Exporter can be used standalone by applying the following yaml. 

```Kubectl apply -f https://raw.githubusercontent.com/CentaurusInfra/alnair/tree/main/alnair-profiler/alnair-exporter/manifests/alnair-exporter.yaml```

Promtheus scrape path and port are already setted in the pod annoation. To configure Prometheus configuration file, only need to select pod by label, refer to the [prometheus-complete.yaml](https://github.com/CentaurusInfra/alnair/blob/main/alnair-profiler/prometheus-complete.yaml#L94).

However, to use cuda-level metrics, we need [Alnair device plugin]() installed to manage Nvidia GPU and intercept cuda APIs.

